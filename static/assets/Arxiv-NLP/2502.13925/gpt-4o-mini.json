{
    "title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual Narratives in Image Sequences?",
    "author": "Xiaochen Wang (Peking University), Heming Xia (Hong Kong Polytechnic University), Jialin Song (Peking University), Longyu Guan (Peking University), Yixin Yang (Peking University), Qingxiu Dong (Peking University), Weiyao Luo (Peking University), Yiru Wang (ModelTC), Yifan Pu (Tsinghua University), Xiangdi Meng (Peking University), Wenjie Li (Hong Kong Polytechnic University), Zhifang Sui (Peking University)",
    "quality": 7,
    "relevance": 6,
    "relevance_why": "The paper explores reasoning over image sequences, which can inform approaches to understanding audio-visual narratives in multimodal contexts, relevant for audio generation tasks like MusicLDM.",
    "field": "Applications-Vision",
    "background": "Understanding narratives from sequences of images, assessing models on how well they can interpret visual stories, predict missing frames, and reorder sequences.",
    "contribution": "This paper introduces STRIPCIPHER, a benchmark for evaluating the reasoning ability of Large Multimodal Models (LMMs) on sequential images, revealing their limitations in comparison to human understanding.",
    "technical_comparison": {
        "prior_work": "Existing benchmarks primarily focused on single images and superficial understanding.",
        "novelty": "This work assesses LMMs through tasks that require reasoning about temporal relationships and context among multiple images."
    },
    "key_innovation": "STRIPCIPHER features comprehensive evaluation tasks that challenge LMMs to comprehend visual narratives and maintain contextual continuity over sequences.",
    "real_world_impact": "The findings indicate significant gaps in AI understanding of sequential image narratives, which could steer future research towards deeper learning methodologies that enhance model capabilities for real-world applications.",
    "limitations": "Limited to comic strips, which may not represent broader visual storytelling contexts.",
    "new_terms": {
        "Large Multimodal Models (LMMs)": "**Large Multimodal Models (LMMs)** are models that integrate and process information from multiple modalities, such as text and images, to perform tasks that require understanding both visual and linguistic data."
    },
    "open_sourcing": ""
}