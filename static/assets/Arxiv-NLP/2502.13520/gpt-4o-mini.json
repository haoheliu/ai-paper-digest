{
    "title": "A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment",
    "author": "Khalid N. Elmadani (New York University Abu Dhabi), Nizar Habash (New York University Abu Dhabi), Hanada Taha-Thomure (Zayed University), ...",
    "quality": 7,
    "relevance": 6,
    "relevance_why": "This work investigates readability assessment and could offer methodologies that help in evaluating the complexity of audio transcripts or related texts, which may be beneficial for training models in audio-language tasks.",
    "field": "Applications-Language",
    "background": "The task involves assessing the readability of Arabic text across multiple educational levels, helping educators better match text complexity with reader capabilities.",
    "contribution": "This paper introduces the Balanced Arabic Readability Evaluation Corpus (BAREC) to solve the challenge of fine-grained Arabic readability assessment, achieving over 68,000 sentences spanning diverse genres and educational levels.",
    "technical_comparison": {
        "prior_work": "Previous methods of evaluating Arabic text readability relied on limited datasets with fewer readability levels, hampering comprehensive assessment.",
        "novelty": "This work improves by providing a large, balanced dataset with high inter-annotator agreement and detailed readability levels, significantly enhancing previous methodologies."
    },
    "key_innovation": "The dataset is uniquely annotated across 19 levels, enabling more precise matching of text complexity to educational needs.",
    "real_world_impact": "The BAREC dataset can assist educators in selecting appropriate texts for students and potentially facilitate the development of automated tools for readability assessment in Arabic, significantly impacting educational practices.",
    "limitations": "The inherent subjectivity in readability assessments may introduce variability in annotations despite rigorous guidelines.",
    "new_terms": {
        "Quadratic Weighted Kappa": "**Quadratic Weighted Kappa** is a statistical measure of inter-rater agreement for qualitative (categorical) items, considering the degree of disagreement between annotators."
    },
    "open_sourcing": "The BAREC dataset will be made openly available along with detailed annotation guidelines and benchmark results."
}