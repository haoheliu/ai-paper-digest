{
    "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs",
    "author": "Paris Koloveas (IMSI, ATHENA RC), Thanasis Vergoulis (IMSI, ATHENA RC), Serafeim Chatzopoulos (IMSI, ATHENA RC), Christos Tryfonopoulos (University of the Peloponnese)",
    "quality": 9,
    "relevance": 7,
    "relevance_why": "The methodologies and findings about fine-tuning language models could inform techniques for improving audio and speech models, especially in the context of citation and academic references.",
    "field": "Applications-Language",
    "background": "Predicting the intent behind citations in academic papers using machine learning models, assessing how different configurations influence performance.",
    "contribution": "This paper introduces the application of open Large Language Models (LLMs) to classify citation intent effectively, achieving significant performance improvements through fine-tuning.",
    "technical_comparison": {
        "prior_work": "Previous approaches relied on domain-specific models like SciBERT, which required extensive training on large scientific datasets.",
        "novelty": "This work demonstrates that general-purpose models can be effectively fine-tuned on minimal data, thus reducing the need for exhaustive pretraining."
    },
    "key_innovation": "Utilizes general-purpose LLMs for citation intent classification, showing that minimal task-specific data can lead to robust classifications without domain-specific models.",
    "real_world_impact": "This research suggests that LLMs can be adapted for diverse language understanding tasks, promoting accessibility in academic writing due to easier deployment and adaptability.",
    "limitations": "Limited evaluation of the latest models due to rapid advancements and focus only on small to medium parameter counts.",
    "new_terms": {
        "in-context learning": "**In-context learning** is a method by which models learn from examples presented in the prompt rather than through traditional supervised training.",
        "supervised fine-tuning": "**Supervised fine-tuning** involves training a pre-trained model on a specific task using labeled data to improve performance on that task."
    },
    "open_sourcing": "Evaluation framework and models made openly available on GitHub and Hugging Face."
}