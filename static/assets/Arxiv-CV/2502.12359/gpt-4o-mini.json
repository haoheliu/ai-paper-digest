{
    "title": "LanP: Rethinking the Impact of Language Priors in Large Vision-Language Models",
    "author": "Zongyu Wu (The Pennsylvania State University), Yuwei Niu (Singapore University of Technology and Design), Hongcheng Gao (Singapore University of Technology and Design), Minhua Lin (The Pennsylvania State University), Zhiwei Zhang (The Pennsylvania State University), Zhifang Zhang (Singapore University of Technology and Design), Qi Shi (Peking University), Yilong Wang (The Pennsylvania State University), ..., Suhang Wang (The Pennsylvania State University)",
    "quality": 7,
    "relevance": 5,
    "relevance_why": "The exploration of language priors could apply insights to improve audio-based tasks by leveraging contextual information similar to the proposed techniques in LVLMs.",
    "field": "Deep Learning-Large Language Models (LLMs)",
    "background": "The study investigates how well large vision-language models leverage language knowledge when visual information is limited or ambiguous.",
    "contribution": "This paper introduces the LanP benchmark to investigate the positive roles of language priors in large vision-language models, highlighting both strengths and challenges.",
    "technical_comparison": {
        "prior_work": "Previous models often underutilized language knowledge in challenging visual question answering scenarios, especially with partially hidden objects.",
        "novelty": "This benchmark allows for a nuanced assessment of how language priors can aid in object recognition and question answering."
    },
    "key_innovation": "It presents a structured approach to quantitatively measure the effectiveness of language priors in various visual contexts.",
    "real_world_impact": "Insights from this study can enhance the design of more robust multimodal models, improving tasks across various domains including healthcare and finance.",
    "limitations": "Some overlap exists between categories in the benchmark, which might lead to ambiguity in evaluation.",
    "new_terms": {
        "language priors": "**Language priors** refer to the knowledge and biases that models derive from textual training data, which can influence their responses in multimodal settings."
    },
    "open_sourcing": ""
}