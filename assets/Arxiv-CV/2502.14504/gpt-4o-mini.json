{
    "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models",
    "author": "Yu Meng (Shenzhen International Graduate School, Tsinghua University), Kaiyuan Li (Shenzhen International Graduate School, Tsinghua University), Chenran Huang (Shenzhen International Graduate School, Tsinghua University), Chen Gao (Tsinghua University), Xinlei Chen (Shenzhen International Graduate School, Tsinghua University), Yong Li (Tsinghua University), Xiaoping Zhang (Shenzhen International Graduate School, Tsinghua University)",
    "quality": 8,
    "relevance": 5,
    "relevance_why": "",
    "field": "Deep Learning-Optimization for Deep Networks",
    "background": "Optimizing the decoding process of large vision-language models by effectively reducing the number of visual tokens processed during inference without a significant loss in performance.",
    "contribution": "This paper introduces Per-Layer Per-Head Vision Token Pruning (PLPHP) to solve the inefficiency in decoding large vision-language models, achieving an 18% speedup and over 50% reduction in Key-Value Cache size with only 0.46% performance degradation.",
    "technical_comparison": {
        "prior_work": "Previous methods commonly implemented fixed token pruning strategies that compromised model performance, usually discarding vision tokens without considering layer and head-specific attention.",
        "novelty": "This work dynamically adjusts pruning rates at both layer and attention head levels based on observed attention patterns, allowing for more efficient pruning and performance retention."
    },
    "key_innovation": "The method uniquely combines layer-level retention rate allocation with head-level pruning, allowing more nuanced selection of retained vision tokens based on attention analysis.",
    "real_world_impact": "Improving the efficiency of large vision-language models can enhance real-time applications in multimedia processing, making these models more accessible and usable on devices with limited computational power.",
    "limitations": "No",
    "new_terms": {
        "Vision Token Re-attention": "**Vision Token Re-attention** refers to the phenomenon where different decoder layers of a model exhibit varying levels of attention to the same visual tokens, suggesting that pruning strategies should consider these differences.",
        "Key-Value Cache": "**Key-Value Cache** is a mechanism used in transformer models to store and retrieve contextual information efficiently during inference for autoregressive generation tasks."
    },
    "open_sourcing": "Our source code will be made publicly available."
}