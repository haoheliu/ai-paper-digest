{
    "title": "SCOPE: A Self-Supervised Framework for Improving Faithfulness in Conditional Text Generation",
    "author": "Song Duong (Sorbonne Universit\u00e9, CNRS), Florian Le Bronnec (Universit\u00e9 Paris-Dauphine), Alexandre Allauzen (Universit\u00e9 Paris-Dauphine), Vincent Guigue (AgroParisTech), Alberto Lumbreras (Criteo AI Lab), Laure Soulier (Sorbonne Universit\u00e9), Patrick Gallinari (Sorbonne Universit\u00e9), ...",
    "quality": 7,
    "relevance": 6,
    "relevance_why": "The methods developed in this paper for improving text generation accuracy through self-supervised learning can inform techniques for text-to-audio alignment tasks related to audio captioning or sound synthesis.",
    "field": "Deep Learning-Large Language Models (LLMs)",
    "background": "Improving the accuracy of text generated by models based on context while preventing the introduction of unrelated or incorrect information.",
    "contribution": "This paper introduces SCOPE to solve the problem of hallucination in text generation, achieving significant improvements in faithfulness metrics over existing methods.",
    "technical_comparison": {
        "prior_work": "Previous methods either relied on external tools for generating unfaithful examples or focused on decoding strategies without addressing the underlying model training.",
        "novelty": "This work combines initial fine-tuning with a novel self-supervised preference-tuning process to enhance model training effectively."
    },
    "key_innovation": "The self-supervised generation of unfaithful examples within the model training allows for improved focus on contextually grounded outputs.",
    "real_world_impact": "Enhancing the reliability of text generation models can have significant implications for applications requiring high accuracy, such as medical transcription and automated reporting.",
    "limitations": "The authors mention limits in domain complexity and emphasize the need for more extensive human evaluations across diverse datasets.",
    "new_terms": {
        "hallucinations": "**Hallucinations** refer to instances when a model generates outputs that include false or ungrounded information not present in the input data.",
        "preference tuning": "**Preference tuning** is a method where models are fine-tuned based on preferences between generated outputs, improving model decision-making."
    },
    "open_sourcing": "Code is available at https://github.com/sngdng/scope-faithfulness"
}