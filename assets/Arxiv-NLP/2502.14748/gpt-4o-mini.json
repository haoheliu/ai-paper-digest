{
    "title": "Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs",
    "author": "Zongxia Li (University of Maryland, College Park), Lorena Calvo-Bartolom\u00e9 (Universidad Carlos III of Madrid, Spain), Alexander Hoyle (University of Maryland, College Park), Daniel Stephens (University of Maryland, College Park), Paiheng Xu (University of Maryland, College Park), Alden Dima (University of Maryland, College Park), Juan Francisco Fung (University of Maryland, College Park), Jordan Boyd-Graber (University of Maryland, College Park)",
    "quality": 8,
    "relevance": 7,
    "relevance_why": "This paper presents insights on the effectiveness of large language models (LLMs) in data exploration which can inform improvements in generating audio and text datasets for model training and evaluation.",
    "field": "Applications-Language",
    "background": "The study evaluates how well large language models assist users in understanding large text corpora compared to traditional topic modeling techniques.",
    "contribution": "This study introduces a human-in-the-loop evaluation framework for comparing LLMs and traditional models, achieving better topic generation and understanding through user input.",
    "technical_comparison": {
        "prior_work": "Previous approaches primarily relied on automated metrics for evaluating topic models.",
        "novelty": "This work integrates human validation for assessing topic outputs, allowing for a better understanding of user interactions with topic models."
    },
    "key_innovation": "Combines traditional topic modeling with interactive LLM-assisted generation to enhance the user experience and topic relevance.",
    "real_world_impact": "Improves tools for data exploration in various domains, potentially enhancing academic research and practical applications in fields requiring large-scale text analysis.",
    "limitations": "The study indicates significant human effort is required in the supervision of LLM processes, potentially limiting scalability.",
    "new_terms": {
        "human-in-the-loop": "**Human-in-the-loop** refers to systems that integrate human feedback into algorithmic processes, enhancing decision-making, especially in complex data scenarios.",
        "hallucination": "**Hallucination** in LLMs refers to the generation of content that is not grounded in the provided input data, leading to potentially misleading or irrelevant information."
    },
    "open_sourcing": "Dataset available at https://huggingface.co/datasets/zli12321/Bills"
}