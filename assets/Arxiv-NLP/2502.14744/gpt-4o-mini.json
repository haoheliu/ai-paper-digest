{
    "title": "HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States",
    "author": "Yilei Jiang (The Chinese University of Hong Kong), Xinyan Gao (The Chinese University of Hong Kong), Tianshuo Peng (The Chinese University of Hong Kong), Yingshui Tan (Alibaba Group), Xiaoyong Zhu (Alibaba Group), Bo Zheng (Alibaba Group), Xiangyu Yue (The Chinese University of Hong Kong)",
    "quality": 8,
    "relevance": 7,
    "relevance_why": "The paper explores internal activation-based safety mechanisms in large vision-language models, which could provide insights into ensuring safety in audio and multimodal environments, relevant to Haohe Liu's work in audio-language modeling.",
    "field": "Applications-Language",
    "background": "The research addresses the detection of unsafe prompts in large vision-language models by analyzing their internal hidden states during inference.",
    "contribution": "HiddenDetect introduces an activation-based framework for detecting unsafe prompts in large vision-language models, achieving enhanced safety monitoring without extensive fine-tuning.",
    "technical_comparison": {
        "prior_work": "Previous methods primarily rely on fine-tuning and reactive adjustments to detect unsafe interactions.",
        "novelty": "This work circumvents the need for fine-tuning by leveraging intrinsic safety signals in the model\u2019s hidden states during inference."
    },
    "key_innovation": "Utilizes distinct internal activation patterns to proactively identify unsafe inputs in real-time, offering a more responsive approach to model safety.",
    "real_world_impact": "The proposed framework can be directly applied to enhance the safety of real-world multimodal AI applications, potentially mitigating risks in sensitive deployments.",
    "limitations": "The authors mention that detection effectiveness may vary depending on the subtlety of adversarial inputs and the need for adaptive learning mechanisms to refine detection thresholds.",
    "new_terms": {
        "Refusal Vector": "**Refusal Vector** is a learned representation that encapsulates safety-related semantic signals to aid in detecting unsafe prompts within model activations."
    },
    "open_sourcing": "The code will be released publicly at https://github.com/leigest519/HiddenDetect"
}